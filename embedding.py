import psycopg2
import numpy as np
import os
from dotenv import load_dotenv
from vertexai.language_models import TextEmbeddingModel
import vertexai
import ast 

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()
PROJECT_ID = os.getenv("GOOGLE_CLOUD_PROJECT")
LOCATION = "us-central1"
MODEL_NAME = "text-embedding-005"

# åˆå§‹åŒ– Vertex AI
vertexai.init(project=PROJECT_ID, location=LOCATION)

# åŠ è¼‰åµŒå…¥æ¨¡å‹
embedding_model = TextEmbeddingModel.from_pretrained(MODEL_NAME)

# PostgreSQL é€£ç·šè¨­å®š
DB_CONFIG = {
    "dbname": "meetings_db",
    "user": "postgres",
    "password": "postgres",
    "host": "127.0.0.1",  # Cloud SQL Proxy é€£ç·š
    "port": "5433"
}

def get_text_embedding(text: str) -> np.ndarray:
    """
    ä½¿ç”¨ Google Vertex AI å–å¾—æ–‡æœ¬çš„åµŒå…¥å‘é‡ (768 ç¶­).
    
    Args:
        text (str): è¦è½‰æ›çš„æ–‡æœ¬
    Returns:
        np.ndarray: 768 ç¶­åµŒå…¥å‘é‡
    """
    # å–å¾—åµŒå…¥
    response = embedding_model.get_embeddings([text])

    # æå–å‘é‡
    embedding_vector = response[0].values  
    embedding_array = np.array(embedding_vector)  # è½‰æ›ç‚º NumPy é™£åˆ—

    return embedding_array  # è¿”å› 768 ç¶­çš„ NumPy å‘é‡

def insert_embedding(text: str):
    """
    å–å¾—æ–‡æœ¬çš„åµŒå…¥å‘é‡ï¼Œä¸¦æ’å…¥åˆ° documents è³‡æ–™è¡¨ä¸­ã€‚
    
    Args:
        text (str): è¦åµŒå…¥çš„æ–‡æœ¬å…§å®¹ã€‚
    """
    # é€£æ¥ Cloud SQL
    conn = psycopg2.connect(**DB_CONFIG)
    cur = conn.cursor()

    try:
        # å–å¾— Google Vertex AI ç”¢ç”Ÿçš„åµŒå…¥å‘é‡
        embedding_array = get_text_embedding(text)

        # ç¢ºä¿æ ¼å¼ç¬¦åˆ PostgreSQL `vector(768)`
        embedding_list = embedding_array.tolist()

        # æ’å…¥è³‡æ–™åˆ° `documents` è¡¨
        cur.execute("""
            INSERT INTO documents (original_text, embedding)
            VALUES (%s, %s);
        """, (text, embedding_list))

        conn.commit()  # ç¢ºä¿æ•¸æ“šæäº¤
        print("âœ… æˆåŠŸæ’å…¥åµŒå…¥å‘é‡ï¼")

    except Exception as e:
        print(f"âŒ æ’å…¥å¤±æ•—: {e}")

    finally:
        cur.close()
        conn.close()


def fetch_all_documents():
    """
    æŸ¥è©¢ `documents` è¡¨ä¸­çš„æ‰€æœ‰è³‡æ–™ï¼Œä¸¦é¡¯ç¤º IDã€åŸå§‹æ–‡æœ¬ã€è™•ç†å¾Œæ–‡æœ¬ã€åµŒå…¥å‘é‡ï¼ˆå‰ 10 ç¶­ï¼‰ã€å»ºç«‹æ™‚é–“ã€‚
    
    Returns:
        list: åŒ…å«æ‰€æœ‰æŸ¥è©¢çµæœçš„åˆ—è¡¨ï¼Œæ¯å€‹å…ƒç´ æ˜¯ (id, original_text, processed_text, embedding_list, created_at)
    """
    # é€£æ¥ PostgreSQL
    conn = psycopg2.connect(**DB_CONFIG)
    cur = conn.cursor()

    try:
        # æŸ¥è©¢æ‰€æœ‰æ•¸æ“š
        cur.execute("SELECT id, original_text, processed_text, embedding, created_at FROM documents;")
        rows = cur.fetchall()  # å–å¾—æ‰€æœ‰è³‡æ–™

        # æ ¼å¼åŒ–çµæœ
        results = []
        for row in rows:
            doc_id = row[0]
            original_text = row[1]
            processed_text = row[2] if row[2] is not None else "ç„¡"  # å¦‚æœç‚º NULLï¼Œé¡¯ç¤º "ç„¡"
            embedding_str = row[3]  # 768 ç¶­å‘é‡ (pgvector)
            created_at = row[4]  # å»ºç«‹æ™‚é–“
            
            # è§£æ pgvector å‘é‡
            if embedding_str:
                try:
                    embedding = ast.literal_eval(embedding_str)  # è§£ææˆ list of float
                    if isinstance(embedding, list):
                        embedding_preview = embedding[:10]  # å–å‰ 10 ç¶­
                    else:
                        embedding_preview = "âŒ è§£æéŒ¯èª¤"
                except:
                    embedding = []
                    embedding_preview = "âŒ è§£æéŒ¯èª¤"
            else:
                embedding = []
                embedding_preview = "âŒ ç„¡å‘é‡"

            # åªé¡¯ç¤ºå‰ 10 ç¶­
            embedding_preview = embedding[:10] if embedding else []

            print(f"ğŸ†” ID: {doc_id}")
            print(f"ğŸ“„ åŸå§‹æ–‡æœ¬: {original_text[:50]}...")  # é¡¯ç¤ºå‰ 50 å€‹å­—
            print(f"ğŸ“ è™•ç†å¾Œæ–‡æœ¬: {processed_text[:50]}..." if processed_text != "ç„¡" else "ğŸ“ è™•ç†å¾Œæ–‡æœ¬: ç„¡")
            print(f"ğŸ§¬ åµŒå…¥å‘é‡ï¼ˆå‰10ç¶­ï¼‰: {embedding_preview} ...")
            print(f"ğŸ“… å»ºç«‹æ™‚é–“: {created_at}\n")

            results.append((doc_id, original_text, processed_text, embedding, created_at))

        return results  # è¿”å›æ‰€æœ‰æ•¸æ“š

    except Exception as e:
        print(f"âŒ æŸ¥è©¢å¤±æ•—: {e}")
        return []

    finally:
        cur.close()
        conn.close()


def find_top_similar_documents(query_text: str, top_k: int = 3):
    """
    åˆ©ç”¨ pgvector æ‰¾åˆ°èˆ‡ `query_text` ç›¸ä¼¼åº¦æœ€é«˜çš„å‰ `top_k` ç­†è³‡æ–™ã€‚
    
    Args:
        query_text (str): è¦æŸ¥è©¢ç›¸ä¼¼åº¦çš„æ–‡æœ¬
        top_k (int): è¦è¿”å›çš„ç›¸ä¼¼çµæœæ•¸é‡ (é è¨­ 3)
    
    Returns:
        list: æœ€ç›¸ä¼¼çš„ `top_k` ç­†çµæœï¼Œæ¯å€‹å…ƒç´ åŒ…å« (original_text, processed_text, similarity_score)
    """
    # å–å¾—æŸ¥è©¢æ–‡æœ¬çš„åµŒå…¥å‘é‡
    query_embedding = get_text_embedding(query_text).tolist()

    # é€£æ¥ PostgreSQL
    conn = psycopg2.connect(**DB_CONFIG)
    cur = conn.cursor()

    try:
        # æŸ¥è©¢æœ€ç›¸ä¼¼çš„å‰ `top_k` ç­†è³‡æ–™
        cur.execute("""
            SELECT original_text, processed_text, embedding <=> %s::vector(768) AS similarity
            FROM documents
            ORDER BY similarity ASC
            LIMIT %s;
        """, (query_embedding, top_k))

        rows = cur.fetchall()  # å–å¾—æŸ¥è©¢çµæœ

        results = []
        for row in rows:
            original_text = row[0]
            processed_text = row[1] if row[1] is not None else "ç„¡"
            similarity_score = row[2]  # ç›¸ä¼¼åº¦åˆ†æ•¸ (è¶Šå°è¶Šç›¸ä¼¼)

            print(f"ğŸ“„ åŸå§‹æ–‡æœ¬: {original_text[:50]}...")
            print(f"ğŸ“ è™•ç†å¾Œæ–‡æœ¬: {processed_text[:50]}..." if processed_text != "ç„¡" else "ğŸ“ è™•ç†å¾Œæ–‡æœ¬: ç„¡")
            print(f"ğŸ¯ ç›¸ä¼¼åº¦åˆ†æ•¸: {similarity_score:.4f}")
            print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

            results.append((original_text, processed_text, similarity_score))

        return results  # è¿”å›æœ€ç›¸ä¼¼çš„ `top_k` ç­†çµæœ

    except Exception as e:
        print(f"âŒ æŸ¥è©¢å¤±æ•—: {e}")
        return []

    finally:
        cur.close()
        conn.close()



# æ¸¬è©¦æ’å…¥æ–‡æœ¬
# text_input = """DDR Ratioï¼ˆDouble Data Rate Ratioï¼‰ æ˜¯ç”¨ä¾†è¡¡é‡è¨˜æ†¶é«”ï¼ˆRAMï¼‰é‹ä½œæ•ˆç‡çš„ä¸€å€‹é—œéµæŒ‡æ¨™ï¼Œç‰¹åˆ¥æ˜¯åœ¨è¨ˆç®—æ©Ÿç³»çµ±å’ŒåµŒå…¥å¼è¨­å‚™ä¸­ã€‚DDR è¨˜æ†¶é«”æŠ€è¡“ï¼ˆå¦‚ DDR3ã€DDR4ã€DDR5ï¼‰é€šéé›™å€æ•¸æ“šé€Ÿç‡å‚³è¼¸ä¾†æé«˜å­˜å–æ•ˆç‡ï¼Œä½¿å¾—æ•¸æ“šåœ¨æ™‚è„ˆ ä¿¡è™Ÿçš„ä¸Šå‡èˆ‡ä¸‹é™æ²¿çš†å¯å‚³è¼¸ï¼Œæé«˜ç¸½é«”çš„è³‡æ–™ååé‡ã€‚

# DDR Ratio é€šå¸¸æŒ‡çš„æ˜¯è¨˜æ†¶é«”çš„æ™‚è„ˆé »ç‡ï¼ˆClock Speedï¼‰èˆ‡å¯¦éš›æ•¸æ“šå‚³è¼¸é€Ÿç‡ï¼ˆData Rateï¼‰ä¹‹é–“çš„æ¯”ç‡ã€‚ä¾‹å¦‚ï¼š

# åœ¨ DDR4-3200 è¨˜æ†¶é«”ä¸­ï¼Œæ™‚è„ˆé »ç‡ç‚º 1600 MHzï¼Œä½†ç”±æ–¼ DDR æŠ€è¡“æ¡ç”¨é›™å€æ•¸æ“šå‚³è¼¸ï¼Œå¯¦éš›çš„æ•¸æ“šé€Ÿç‡ç‚º 3200 MT/sï¼ˆMega Transfers per secondï¼‰ï¼Œå› æ­¤ DDR Ratio é€šå¸¸è¡¨ç¤ºç‚º 2:1ã€‚
# å°æ–¼ DDR5 è¨˜æ†¶é«”ï¼Œéš¨è‘—æŠ€è¡“é€²æ­¥ï¼Œæ•¸æ“šå‚³è¼¸æ•ˆç‡é€²ä¸€æ­¥æå‡ï¼Œä½†ä»ç„¶ä¿æŒç›¸ä¼¼çš„æ¯”ä¾‹é—œä¿‚ã€‚
# åœ¨æ•ˆèƒ½æ¸¬è©¦æˆ–è¶…é »èª¿æ ¡æ™‚ï¼ŒDDR Ratio å¯èƒ½å½±éŸ¿ç³»çµ±ç©©å®šæ€§èˆ‡é »å¯¬è¡¨ç¾ï¼Œå› æ­¤èª¿æ•´è¨˜æ†¶é«”åƒæ•¸æ™‚éœ€è¦è€ƒé‡æ­¤æ¯”ä¾‹ï¼Œä»¥ç¢ºä¿æœ€ä½³çš„é‹è¡Œæ•ˆèƒ½å’Œç©©å®šæ€§ã€‚"""
# insert_embedding(text_input)

#æŸ¥è©¢æ‰€æœ‰æ–‡æª”
# fetch_all_documents()

# # æŸ¥è©¢ç›¸ä¼¼æ–‡æª”
# query_text = """"""
# find_top_similar_documents(query_text, top_k=3)
